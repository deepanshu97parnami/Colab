{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLAssignment.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO7PWvg266gWxmgaMdwJzUn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepanshu97parnami/Colab/blob/main/MLAssignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSZNXY5rCYny"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "  \n",
        "pd.options.mode.chained_assignment = None\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "link = 'https://drive.google.com/file/d/1Ou9NRiNOxQ1h5CgQ4VPk-_q33bkdOuCR/view?usp=sharing'\n",
        "  \n",
        "#import pandas as pd\n",
        "  \n",
        "# to get the id part of the file\n",
        "id = link.split(\"/\")[-2]\n",
        "  \n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('train_v2.csv')  \n",
        "\n",
        "train = pd.read_csv('train_v2.csv')\n",
        "\n",
        "\n",
        "\n",
        "link = 'https://drive.google.com/file/d/1s9ujwXgLwWkR7wtvhSMl5Ngv7ug-BxVM/view?usp=sharing'\n",
        "  \n",
        "#import pandas as pd\n",
        "  \n",
        "# to get the id part of the file\n",
        "id1 = link.split(\"/\")[-2]\n",
        "  \n",
        "downloaded = drive.CreateFile({'id':id1}) \n",
        "downloaded.GetContentFile('sample_submission_v2.csv')  \n",
        "\n",
        "sample_submission = pd.read_csv('sample_submission_v2.csv')\n",
        "\n",
        "\n",
        "\n",
        "link = 'https://drive.google.com/file/d/1PQ89UVrm9-02TJRuULM62zTTmLrkbN3Q/view?usp=sharing'\n",
        "  \n",
        "#import pandas as pd\n",
        "  \n",
        "# to get the id part of the file\n",
        "id2 = link.split(\"/\")[-2]\n",
        "  \n",
        "downloaded = drive.CreateFile({'id':id2}) \n",
        "downloaded.GetContentFile('transactions_v2.csv')  \n",
        "\n",
        "transactions = pd.read_csv('transactions_v2.csv')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "link = 'https://drive.google.com/file/d/1y8nDre1OyvmPW4P9a3q61-Ui9ktzV_Ix/view?usp=sharing'\n",
        "  \n",
        "#import pandas as pd\n",
        "  \n",
        "# to get the id part of the file\n",
        "id3 = link.split(\"/\")[-2]\n",
        "  \n",
        "downloaded = drive.CreateFile({'id':id3}) \n",
        "downloaded.GetContentFile('user_logs_v2.csv')  \n",
        "\n",
        "user_logs = pd.read_csv('user_logs_v2.csv')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "link = 'https://drive.google.com/file/d/1K4BiItsbuFwmhdW82riPlkLYSDlFeADD/view?usp=sharing'\n",
        "  \n",
        "\n",
        "id4 = link.split(\"/\")[-2]\n",
        "  \n",
        "downloaded = drive.CreateFile({'id':id4}) \n",
        "downloaded.GetContentFile('members_v3.csv')  \n",
        "\n",
        "members = pd.read_csv('members_v3.csv')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# set the options so the output format can be displayed correctly\n",
        "pd.set_option('expand_frame_repr', True)\n",
        "pd.set_option('display.max_rows', 30000000)\n",
        "pd.set_option('display.max_columns', 100)\n",
        "\n",
        "# check the number of duplicate accounts in each table\n",
        "train.duplicated('msno').sum()\n",
        "sample_submission.duplicated('msno').sum()\n",
        "transactions.duplicated('msno').sum()\n",
        "user_logs.duplicated('msno').sum()\n",
        "members.duplicated('msno').sum()\n",
        "\n",
        "# returns the max value of numerical variables and membership_expire_date\n",
        "# returns the min value of transaction date\n",
        "# returns the mode of ordinal variable and dummy variables, if multiple values share the same frequency, keep the first one\n",
        "transactions_v2 = transactions.groupby('msno', as_index = False).agg({'payment_method_id': lambda x:x.value_counts().index[0], 'payment_plan_days': 'max', 'plan_list_price': 'max',\n",
        "                                       'actual_amount_paid': 'max', 'is_auto_renew': lambda x:x.value_counts().index[0], 'transaction_date': 'min', 'membership_expire_date': 'max',\n",
        "                                       'is_cancel': lambda x:x.value_counts().index[0]})\n",
        "\n",
        "# returns the max value of date and number of unique songs\n",
        "# returns the sum of other variables\n",
        "user_logs_v2 = user_logs.groupby('msno', as_index = False).agg({'date': 'max', 'num_25': 'sum', 'num_50': 'sum', 'num_75': 'sum',\n",
        "                                 'num_985': 'sum', 'num_100': 'sum', 'num_unq': 'max', 'total_secs': 'sum'})\n",
        "\n",
        "# calculate the percentage of number of songs played within certain period\n",
        "user_logs_v2['percent_25'] = user_logs_v2['num_25']/(user_logs_v2['num_25']+user_logs_v2['num_50']+user_logs_v2['num_75']+user_logs_v2['num_985']+user_logs_v2['num_100'])\n",
        "user_logs_v2['percent_50'] = user_logs_v2['num_50']/(user_logs_v2['num_25']+user_logs_v2['num_50']+user_logs_v2['num_75']+user_logs_v2['num_985']+user_logs_v2['num_100'])\n",
        "user_logs_v2['percent_100'] = (user_logs_v2['num_985']+user_logs_v2['num_100'])/(user_logs_v2['num_25']+user_logs_v2['num_50']+user_logs_v2['num_75']+user_logs_v2['num_985']+user_logs_v2['num_100'])\n",
        "\n",
        "# drop useless variables\n",
        "user_logs_v3 = user_logs_v2.drop(columns = ['num_25', 'num_50', 'num_75', 'num_985', 'num_100'])\n",
        "\n",
        "# merge between different tables for modelling purpose\n",
        "dataset_train = train.merge(members, on = 'msno', how = 'left').merge(transactions_v2, on = 'msno', how = 'left').merge(user_logs_v3, on = 'msno', how = 'left')\n",
        "dataset_train.dtypes\n",
        "\n",
        "# date in csv will be recognized as float in python\n",
        "# this value needs to be converted back to date\n",
        "dataset_train['registration_init_time'] = pd.to_datetime(dataset_train['registration_init_time'], format = '%Y%m%d')\n",
        "dataset_train['transaction_date'] = pd.to_datetime(dataset_train['transaction_date'], format = '%Y%m%d')\n",
        "dataset_train['membership_expire_date'] = pd.to_datetime(dataset_train['membership_expire_date'], format = '%Y%m%d')\n",
        "dataset_train['date'] = pd.to_datetime(dataset_train['date'], format = '%Y%m%d')\n",
        "\n",
        "# check the maximum of datetime value\n",
        "dataset_train.select_dtypes(include = ['datetime64[ns]']).max()\n",
        "\n",
        "# create new day columns for modelling purpose\n",
        "dataset_train['registration_day'] = (dataset_train['membership_expire_date'].max() - dataset_train['registration_init_time']).astype('timedelta64[D]')\n",
        "dataset_train['transaction_day'] = (dataset_train['membership_expire_date'].max() - dataset_train['transaction_date']).astype('timedelta64[D]')\n",
        "dataset_train['membership_expire_day'] = (dataset_train['membership_expire_date'].max() - dataset_train['membership_expire_date']).astype('timedelta64[D]')\n",
        "dataset_train['last_play_day'] = (dataset_train['membership_expire_date'].max() - dataset_train['date']).astype('timedelta64[D]')\n",
        "\n",
        "# check the distribution of age due to the documentation explanation\n",
        "dataset_train['bd'].value_counts()\n",
        "\n",
        "# remove gender and age since missing value or incorrect value is over 50%\n",
        "dataset_train_v2 = dataset_train.drop(columns = ['msno', 'gender', 'bd', 'registration_init_time', 'transaction_date', 'membership_expire_date', 'date'])\n",
        "dataset_train_v2.dtypes\n",
        "\n",
        "# check the number of missing values in each column\n",
        "dataset_train_v2.isna().sum()\n",
        "\n",
        "# Handle missing value of part of numeric columns by using mode\n",
        "def replacemode(i):\n",
        "    dataset_train_v2[i] = dataset_train_v2[i].fillna(dataset_train_v2[i].value_counts().index[0])\n",
        "    return \n",
        "\n",
        "replacemode('city')\n",
        "replacemode('registered_via')\n",
        "replacemode('payment_method_id')\n",
        "replacemode('payment_plan_days')\n",
        "replacemode('is_auto_renew')\n",
        "replacemode('is_cancel')\n",
        "\n",
        "# Handle missing value of part of numeric columns by using mean\n",
        "from sklearn.impute import SimpleImputer\n",
        "mean_imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
        "def replacemean(i):\n",
        "    dataset_train_v2[i] = mean_imputer.fit_transform(dataset_train_v2[[i]])\n",
        "    return \n",
        "\n",
        "replacemean('plan_list_price')\n",
        "replacemean('actual_amount_paid')\n",
        "replacemean('num_unq')\n",
        "replacemean('total_secs')\n",
        "replacemean('percent_25')\n",
        "replacemean('percent_50')\n",
        "replacemean('percent_100')\n",
        "replacemean('registration_day')\n",
        "replacemean('transaction_day')\n",
        "replacemean('membership_expire_day')\n",
        "replacemean('last_play_day')\n",
        "\n",
        "# Handle outliers by using capping\n",
        "def replaceoutlier(i):\n",
        "    mean, std = np.mean(dataset_train_v2[i]), np.std(dataset_train_v2[i])\n",
        "    cut_off = std*3\n",
        "    lower, upper = mean - cut_off, mean + cut_off\n",
        "    dataset_train_v2[i][dataset_train_v2[i] < lower] = lower\n",
        "    dataset_train_v2[i][dataset_train_v2[i] > upper] = upper\n",
        "    return\n",
        "\n",
        "replaceoutlier('plan_list_price')\n",
        "replaceoutlier('actual_amount_paid')\n",
        "replaceoutlier('num_unq')\n",
        "replaceoutlier('total_secs')\n",
        "replaceoutlier('percent_25')\n",
        "replaceoutlier('percent_50')\n",
        "replaceoutlier('percent_100')\n",
        "replaceoutlier('registration_day')\n",
        "replaceoutlier('transaction_day')\n",
        "replaceoutlier('membership_expire_day')\n",
        "replaceoutlier('last_play_day')\n",
        "\n",
        "dataset_train_v2.dtypes\n",
        "dataset_train_v2.describe()\n",
        "\n",
        "# convert categorical variables into string\n",
        "dataset_train_v2.iloc[:, 1:4] = dataset_train_v2.iloc[:, 1:4].astype(str)\n",
        "\n",
        "# replace discrete features with historical churn rate\n",
        "city_mean = pd.DataFrame(dataset_train_v2.groupby('city')['is_churn'].mean().reset_index())\n",
        "city_mean.rename(columns = {'is_churn': 'city_mean'}, inplace=True)\n",
        "register_mean = pd.DataFrame(dataset_train_v2.groupby('registered_via')['is_churn'].mean().reset_index())\n",
        "register_mean.rename(columns = {'is_churn': 'register_mean'}, inplace=True)\n",
        "payment_mean = pd.DataFrame(dataset_train_v2.groupby('payment_method_id')['is_churn'].mean().reset_index())\n",
        "payment_mean.rename(columns = {'is_churn': 'payment_mean'}, inplace=True)\n",
        "\n",
        "dataset_train_v3 = dataset_train_v2.merge(city_mean, on = 'city', how = 'left').merge(register_mean, on = 'registered_via', how = 'left').merge(payment_mean, on = 'payment_method_id', how = 'left')\n",
        "dataset_train_v3 = dataset_train_v3.drop(columns = ['city', 'registered_via', 'payment_method_id'])\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "\n",
        "X = dataset_train_v3.drop(columns = ['is_churn'])\n",
        "Y = dataset_train_v3['is_churn']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0)\n",
        "nm_models_pre = []\n",
        "nm_models_pre.append(('KNN', KNeighborsClassifier()))\n",
        "nm_models_pre.append(('LR', LogisticRegression()))\n",
        "nm_models_pre.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "nm_models_pre.append(('CART', DecisionTreeClassifier()))\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "results = []\n",
        "names = []\n",
        "for name, model in nm_models_pre:\n",
        " nm_cv_results = cross_val_score(model, X_train, Y_train, cv=10, scoring='neg_log_loss', n_jobs = -1)\n",
        " model.fit(X_train,Y_train);\n",
        " y_pred = model.predict(X_test)\n",
        " cm = confusion_matrix(Y_test, y_pred)\n",
        " acc = accuracy_score(Y_test, y_pred)\n",
        " print(cm)\n",
        " print(acc)\n",
        " results.append(nm_cv_results)\n",
        " names.append(name)\n",
        " msg = \"%s: %f (%f)\" % (name, nm_cv_results.mean(), nm_cv_results.std())\n",
        " print(msg)\n",
        "\t\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "classifier = Sequential()\n",
        "classifier.add(Dense(units = 6, activation = 'relu'))\n",
        "classifier.add(Dense(units = 6, activation = 'relu'))\n",
        "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "classifier.compile(optimizer = 'Adam', loss ='binary_crossentropy', metrics = ['accuracy'])\n",
        "classifier.fit(X_train, Y_train, batch_size = 10, epochs = 5)\n",
        "y_pred = classifier.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(Y_test, y_pred)\n",
        "print(cm)\n",
        "acc = accuracy_score(Y_test, y_pred)\n",
        "print(acc)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Feature Scaling for modelling purpose by using both min-max-scaling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "nm_X = pd.DataFrame(MinMaxScaler().fit_transform(X))\n",
        "nm_X.columns = X.columns.values\n",
        "nm_X.index = X.index.values\n",
        "\n",
        "# Feature Selection\n",
        "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
        "nm_col = ['is_auto_renew', 'is_cancel']\n",
        "nm_X_v2 = nm_X.drop(columns = nm_col)\n",
        "nm_X_v3 = pd.DataFrame(nm_X, columns = nm_col)\n",
        "nm_X_v4 = pd.DataFrame(SelectKBest(score_func=chi2, k='all').fit(nm_X_v3, Y).pvalues_ <= 0.05, columns = ['importance'])\n",
        "nm_X_v4.index = nm_X_v3.columns.values\n",
        "nm_X_v5 = pd.DataFrame(SelectKBest(score_func=f_classif, k='all').fit(nm_X_v2, Y).pvalues_ <= 0.05, columns = ['importance'])\n",
        "nm_X_v5.index = nm_X_v2.columns.values\n",
        "nm_X_v6 = pd.concat([nm_X_v4, nm_X_v5])\n",
        "nm_selected = list(pd.Series(nm_X_v6[nm_X_v6['importance'] == 1].index.values))\n",
        "nm_X_v7 = pd.DataFrame(nm_X, columns = nm_selected)\n",
        "\n",
        "# Dimension Reduction\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "pca.fit_transform(nm_X_v7)\n",
        "np.cumsum(pca.explained_variance_ratio_)\n",
        "nm_X_v8 = PCA(n_components=10).fit_transform(nm_X_v7)\n",
        "\n",
        "# Split into train and test Set\n",
        "from sklearn.model_selection import train_test_split\n",
        "nm_X_train, nm_X_test, nm_Y_train, nm_Y_test = train_test_split(nm_X_v8, Y, test_size = 0.3, random_state = 0)\n",
        "\n",
        "# Fit training set into different algorithms\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "nm_models = []\n",
        "\n",
        "nm_models.append(('LR', LogisticRegression()))\n",
        "nm_models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "\n",
        "ensembles = []\n",
        "\n",
        "ensembles.append(('XGB', XGBClassifier()))\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "results = []\n",
        "names = []\n",
        "for name, model in nm_models:\n",
        "  nm_cv_results = cross_val_score(model, nm_X_train, nm_Y_train, cv=10, scoring='neg_log_loss', n_jobs = -1)\n",
        "  model.fit(nm_X_train,nm_Y_train)\n",
        "  results.append(nm_cv_results)\n",
        "  nm_y_pred = model.predict(nm_X_test)\n",
        "  cm = confusion_matrix(nm_Y_test, nm_y_pred)\n",
        "  acc = accuracy_score(nm_Y_test, nm_y_pred)\n",
        "  print(acc)\n",
        "  print(cm)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, nm_cv_results.mean(), nm_cv_results.std())\n",
        "  print(msg)\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "nm_X_train = sc.fit_transform(nm_X_train)\n",
        "nm_X_test = sc.transform(nm_X_test)\n",
        "classifier = Sequential()\n",
        "classifier.add(Dense(units = 6, activation = 'relu'))\n",
        "classifier.add(Dense(units = 6, activation = 'relu'))\n",
        "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "classifier.compile(optimizer = 'Adam', loss ='binary_crossentropy', metrics = ['accuracy'])\n",
        "classifier.fit(nm_X_train, nm_Y_train, batch_size = 10, epochs = 5)\n",
        "y_pred = classifier.predict(nm_X_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(nm_Y_test, y_pred)\n",
        "print(cm)\n",
        "acc = accuracy_score(nm_Y_test, y_pred)\n",
        "print(acc)\n",
        "\t\n",
        "\t\n",
        "    \n",
        "results2 = []\n",
        "names2 = []\n",
        "for name2, model2 in ensembles:\n",
        "  en_cv_results = cross_val_score(model2, nm_X_train, nm_Y_train, cv=10, scoring='neg_log_loss', n_jobs = -1)\n",
        "  results2.append(en_cv_results)\n",
        "  names2.append(name2)\n",
        "  nm_y_pred = model.predict(nm_X_test)\n",
        "  cm = confusion_matrix(nm_Y_test, nm_y_pred)\n",
        "  acc = accuracy_score(nm_Y_test, nm_y_pred)\n",
        "  print(acc)\n",
        "  print(cm)\n",
        "  msg2 = \"%s: %f (%f)\" % (name2, en_cv_results.mean(), en_cv_results.std())\n",
        "  print(msg2)\n",
        "\n",
        "# Apply Grid Search on XGBoost since it returns the best result on Cross Validation among all models\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters = {\"learning_rate\": [0.1],\n",
        "              \"min_child_weight\": [1],\n",
        "              \"max_depth\": [9],\n",
        "              \"gamma\": [0.1],\n",
        "              \"subsample\": [0.8],\n",
        "              \"colsample_bytree\": [0.8],\n",
        "              \"objective\": ['binary:logistic']}\n",
        "grid_search_XGB = GridSearchCV(estimator = XGBClassifier(), param_grid = parameters, scoring = \"neg_log_loss\", cv = 10, n_jobs = -1)\n",
        "grid_result_XGB = grid_search_XGB.fit(nm_X_train, nm_Y_train)\n",
        "print(\"Best: %f using %s\" % (grid_result_XGB.best_score_, grid_result_XGB.best_params_)) \n",
        "\n",
        "# Evaluate tuned XGBoost model result on test dataset because it provides the best result\n",
        "from sklearn.metrics import log_loss\n",
        "nm_Y_predict = grid_result_XGB.predict_proba(nm_X_test)\n",
        "logloss = log_loss(nm_Y_test, nm_Y_predict)  \n",
        "print(\"Log Loss Score: %s\" % (logloss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEIYHi08ijqA"
      },
      "source": [
        "X = dataset_train_v3.drop(columns = ['is_churn'])\n",
        "Y = dataset_train_v3['is_churn']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0)\n",
        "nm_models_pre = []\n",
        "nm_models_pre.append(('KNN', KNeighborsClassifier()))\n",
        "nm_models_pre.append(('LR', LogisticRegression()))\n",
        "nm_models_pre.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "nm_models_pre.append(('CART', DecisionTreeClassifier()))\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "results = []\n",
        "names = []\n",
        "for name, model in nm_models_pre:\n",
        " nm_cv_results = cross_val_score(model, X_train, Y_train, cv=10, scoring='neg_log_loss', n_jobs = -1)\n",
        " model.fit(X_train,Y_train);\n",
        " y_pred = model.predict(X_test)\n",
        " cm = confusion_matrix(Y_test, y_pred)\n",
        " acc = accuracy_score(Y_test, y_pred)\n",
        " print(cm)\n",
        " print(acc)\n",
        " results.append(nm_cv_results)\n",
        " names.append(name)\n",
        " svc = svm.SVC(C=1, kernel='linear')\n",
        " clf = svc.fit(X, Y)\n",
        " #predicted = cross_val_predict(clf,X, Y, cv=2)\n",
        " print \"AUC&ROC\",metrics.roc_auc_score(Y, y_pred)\n",
        " msg = \"%s: %f (%f)\" % (name, nm_cv_results.mean(), nm_cv_results.std())\n",
        " print(msg)\n",
        " \n",
        "\t\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy\n",
        "#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "classifier = Sequential()\n",
        "classifier.add(Dense(units = 6, activation = 'relu'))\n",
        "classifier.add(Dense(units = 6, activation = 'relu'))\n",
        "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "classifier.compile(optimizer = 'Adam', loss ='binary_crossentropy', metrics = ['accuracy'])\n",
        "classifier.fit(X_train, Y_train, batch_size = 10, epochs = 5)\n",
        "y_pred = classifier.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(Y_test, y_pred)\n",
        "print(cm)\n",
        "acc = accuracy_score(Y_test, y_pred)\n",
        "print(acc)\n",
        "svc = svm.SVC(C=1, kernel='linear')\n",
        "clf = svc.fit(X, Y)\n",
        "#predicted = cross_val_predict(clf,X, Y, cv=2)\n",
        "print \"AUC&ROC\",metrics.roc_auc_score(Y, y_pred)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Feature Scaling for modelling purpose by using both min-max-scaling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "nm_X = pd.DataFrame(MinMaxScaler().fit_transform(X))\n",
        "nm_X.columns = X.columns.values\n",
        "nm_X.index = X.index.values\n",
        "\n",
        "# Feature Selection\n",
        "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
        "nm_col = ['is_auto_renew', 'is_cancel']\n",
        "nm_X_v2 = nm_X.drop(columns = nm_col)\n",
        "nm_X_v3 = pd.DataFrame(nm_X, columns = nm_col)\n",
        "nm_X_v4 = pd.DataFrame(SelectKBest(score_func=chi2, k='all').fit(nm_X_v3, Y).pvalues_ <= 0.05, columns = ['importance'])\n",
        "nm_X_v4.index = nm_X_v3.columns.values\n",
        "nm_X_v5 = pd.DataFrame(SelectKBest(score_func=f_classif, k='all').fit(nm_X_v2, Y).pvalues_ <= 0.05, columns = ['importance'])\n",
        "nm_X_v5.index = nm_X_v2.columns.values\n",
        "nm_X_v6 = pd.concat([nm_X_v4, nm_X_v5])\n",
        "nm_selected = list(pd.Series(nm_X_v6[nm_X_v6['importance'] == 1].index.values))\n",
        "nm_X_v7 = pd.DataFrame(nm_X, columns = nm_selected)\n",
        "\n",
        "# Dimension Reduction\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "pca.fit_transform(nm_X_v7)\n",
        "np.cumsum(pca.explained_variance_ratio_)\n",
        "nm_X_v8 = PCA(n_components=10).fit_transform(nm_X_v7)\n",
        "\n",
        "# Split into train and test Set\n",
        "from sklearn.model_selection import train_test_split\n",
        "nm_X_train, nm_X_test, nm_Y_train, nm_Y_test = train_test_split(nm_X_v8, Y, test_size = 0.3, random_state = 0)\n",
        "\n",
        "# Fit training set into different algorithms\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "nm_models = []\n",
        "nm_models.append(('LR', LogisticRegression()))\n",
        "nm_models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "\n",
        "ensembles = []\n",
        "ensembles.append(('XGB', XGBClassifier()))\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "results = []\n",
        "names = []\n",
        "for name, model in nm_models:\n",
        "  nm_cv_results = cross_val_score(model, nm_X_train, nm_Y_train, cv=10, scoring='neg_log_loss', n_jobs = -1)\n",
        "  model.fit(nm_X_train,nm_Y_train)\n",
        "  results.append(nm_cv_results)\n",
        "  nm_y_pred = model.predict(nm_X_test)\n",
        "  cm = confusion_matrix(nm_Y_test, nm_y_pred)\n",
        "  acc = accuracy_score(nm_Y_test, nm_y_pred)\n",
        "  print(acc)\n",
        "  print(cm)\n",
        "  names.append(name)\n",
        "  svc = svm.SVC(C=1, kernel='linear')\n",
        "  clf = svc.fit(nm_X_v8, Y)\n",
        "  print \"AUC&ROC\",metrics.roc_auc_score(Y, nm_y_pred)\n",
        "  msg = \"%s: %f (%f)\" % (name, nm_cv_results.mean(), nm_cv_results.std())\n",
        "  print(msg)\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "nm_X_train = sc.fit_transform(nm_X_train)\n",
        "nm_X_test = sc.transform(nm_X_test)\n",
        "classifier = Sequential()\n",
        "classifier.add(Dense(units = 6, activation = 'relu'))\n",
        "classifier.add(Dense(units = 6, activation = 'relu'))\n",
        "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "classifier.compile(optimizer = 'Adam', loss ='binary_crossentropy', metrics = ['accuracy'])\n",
        "classifier.fit(nm_X_train, nm_Y_train, batch_size = 10, epochs = 5)\n",
        "y_pred = classifier.predict(nm_X_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(nm_Y_test, y_pred)\n",
        "print(cm)\n",
        "acc = accuracy_score(nm_Y_test, y_pred)\n",
        "print(acc)\n",
        "svc = svm.SVC(C=1, kernel='linear')\n",
        "clf = svc.fit(nm_X_v8, Y)\n",
        "print \"AUC&ROC\",metrics.roc_auc_score(Y, nm_y_pred)\n",
        "\t\n",
        "\t\n",
        "    \n",
        "results2 = []\n",
        "names2 = []\n",
        "for name2, model2 in ensembles:\n",
        "  en_cv_results = cross_val_score(model2, nm_X_train, nm_Y_train, cv=10, scoring='neg_log_loss', n_jobs = -1)\n",
        "  results2.append(en_cv_results)\n",
        "  nm_y_pred = model.predict(nm_X_test)\n",
        "  cm = confusion_matrix(nm_Y_test, nm_y_pred)\n",
        "  acc = accuracy_score(nm_Y_test, nm_y_pred)\n",
        "  print(acc)\n",
        "  print(cm)\n",
        "  names2.append(name2)\n",
        "  msg2 = \"%s: %f (%f)\" % (name2, en_cv_results.mean(), en_cv_results.std())\n",
        "  print(msg2)\n",
        "\n",
        "# Apply Grid Search on XGBoost since it returns the best result on Cross Validation among all models\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters = {\"learning_rate\": [0.1],\n",
        "              \"min_child_weight\": [1],\n",
        "              \"max_depth\": [9],\n",
        "              \"gamma\": [0.1],\n",
        "              \"subsample\": [0.8],\n",
        "              \"colsample_bytree\": [0.8],\n",
        "              \"objective\": ['binary:logistic']}\n",
        "grid_search_XGB = GridSearchCV(estimator = XGBClassifier(), param_grid = parameters, scoring = \"neg_log_loss\", cv = 10, n_jobs = -1)\n",
        "grid_result_XGB = grid_search_XGB.fit(nm_X_train, nm_Y_train)\n",
        "print(\"Best: %f using %s\" % (grid_result_XGB.best_score_, grid_result_XGB.best_params_)) \n",
        "\n",
        "# Evaluate tuned XGBoost model result on test dataset because it provides the best result\n",
        "from sklearn.metrics import log_loss\n",
        "nm_Y_predict = grid_result_XGB.predict_proba(nm_X_test)\n",
        "logloss = log_loss(nm_Y_test, nm_Y_predict)  \n",
        "print(\"Log Loss Score: %s\" % (logloss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVePUQuuyCHG",
        "outputId": "bcb261d3-9006-4165-8475-c2cf74c95879"
      },
      "source": [
        "results2 = []\n",
        "names2 = []\n",
        "for name2, model2 in ensembles:\n",
        "  en_cv_results = cross_val_score(model2, nm_X_train, nm_Y_train, cv=10, scoring='neg_log_loss', n_jobs = -1)\n",
        "  results2.append(en_cv_results)\n",
        "  nm_y_pred = model.predict(nm_X_test)\n",
        "  cm = confusion_matrix(nm_Y_test, nm_y_pred)\n",
        "  acc = accuracy_score(nm_Y_test, nm_y_pred)\n",
        "  print(acc)\n",
        "  print(cm)\n",
        "  names2.append(name2)\n",
        "  msg2 = \"%s: %f (%f)\" % (name2, en_cv_results.mean(), en_cv_results.std())\n",
        "  print(msg2)\n",
        "\n",
        "# Apply Grid Search on XGBoost since it returns the best result on Cross Validation among all models\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters = {\"learning_rate\": [0.1],\n",
        "              \"min_child_weight\": [1],\n",
        "              \"max_depth\": [9],\n",
        "              \"gamma\": [0.1],\n",
        "              \"subsample\": [0.8],\n",
        "              \"colsample_bytree\": [0.8],\n",
        "              \"objective\": ['binary:logistic']}\n",
        "grid_search_XGB = GridSearchCV(estimator = XGBClassifier(), param_grid = parameters, scoring = \"neg_log_loss\", cv = 10, n_jobs = -1)\n",
        "grid_result_XGB = grid_search_XGB.fit(nm_X_train, nm_Y_train)\n",
        "print(\"Best: %f using %s\" % (grid_result_XGB.best_score_, grid_result_XGB.best_params_)) \n",
        "\n",
        "# Evaluate tuned XGBoost model result on test dataset because it provides the best result\n",
        "from sklearn.metrics import log_loss\n",
        "nm_Y_predict = grid_result_XGB.predict_proba(nm_X_test)\n",
        "logloss = log_loss(nm_Y_test, nm_Y_predict)  \n",
        "print(\"Log Loss Score: %s\" % (logloss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8891955727664717\n",
            "[[240350  24742]\n",
            " [  7534  18662]]\n",
            "XGB: -0.100784 (0.001614)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLAssignment.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPAy7VwMFz3yekyHqTdPLaJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepanshu97parnami/Colab/blob/main/MLAssignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSZNXY5rCYny",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af28731e-126f-455e-f62c-f279331a871c"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "from sklearn.svm import SVC\n",
        "import sklearn\n",
        "from sklearn import svm\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "link = 'https://drive.google.com/file/d/1Ou9NRiNOxQ1h5CgQ4VPk-_q33bkdOuCR/view?usp=sharing'\n",
        "  \n",
        "  \n",
        "# to get the id part of the file\n",
        "id = link.split(\"/\")[-2]\n",
        "  \n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('train_v2.csv')  \n",
        "\n",
        "train = pd.read_csv('train_v2.csv')\n",
        "\n",
        "\n",
        "\n",
        "link = 'https://drive.google.com/file/d/1s9ujwXgLwWkR7wtvhSMl5Ngv7ug-BxVM/view?usp=sharing'\n",
        "  \n",
        "  \n",
        "# to get the id part of the file\n",
        "id1 = link.split(\"/\")[-2]\n",
        "  \n",
        "downloaded = drive.CreateFile({'id':id1}) \n",
        "downloaded.GetContentFile('sample_submission_v2.csv')  \n",
        "\n",
        "sample_submission = pd.read_csv('sample_submission_v2.csv')\n",
        "\n",
        "\n",
        "\n",
        "link = 'https://drive.google.com/file/d/1PQ89UVrm9-02TJRuULM62zTTmLrkbN3Q/view?usp=sharing'\n",
        "  \n",
        "  \n",
        "# to get the id part of the file\n",
        "id2 = link.split(\"/\")[-2]\n",
        "  \n",
        "downloaded = drive.CreateFile({'id':id2}) \n",
        "downloaded.GetContentFile('transactions_v2.csv')  \n",
        "\n",
        "transactions = pd.read_csv('transactions_v2.csv')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "link = 'https://drive.google.com/file/d/1y8nDre1OyvmPW4P9a3q61-Ui9ktzV_Ix/view?usp=sharing'\n",
        "  \n",
        "  \n",
        "# to get the id part of the file\n",
        "id3 = link.split(\"/\")[-2]\n",
        "  \n",
        "downloaded = drive.CreateFile({'id':id3}) \n",
        "downloaded.GetContentFile('user_logs_v2.csv')  \n",
        "\n",
        "user_logs = pd.read_csv('user_logs_v2.csv')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "link = 'https://drive.google.com/file/d/1K4BiItsbuFwmhdW82riPlkLYSDlFeADD/view?usp=sharing'\n",
        "  \n",
        "\n",
        "id4 = link.split(\"/\")[-2]\n",
        "  \n",
        "downloaded = drive.CreateFile({'id':id4}) \n",
        "downloaded.GetContentFile('members_v3.csv')  \n",
        "\n",
        "members = pd.read_csv('members_v3.csv')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# set the options so the output format can be displayed correctly\n",
        "pd.set_option('expand_frame_repr', True)\n",
        "pd.set_option('display.max_rows', 30000000)\n",
        "pd.set_option('display.max_columns', 100)\n",
        "\n",
        "# check the number of duplicate accounts in each table\n",
        "train.duplicated('msno').sum()\n",
        "sample_submission.duplicated('msno').sum()\n",
        "transactions.duplicated('msno').sum()\n",
        "user_logs.duplicated('msno').sum()\n",
        "members.duplicated('msno').sum()\n",
        "\n",
        "# returns the max value of numerical variables and membership_expire_date\n",
        "# returns the min value of transaction date\n",
        "# returns the mode of ordinal variable and dummy variables, if multiple values share the same frequency, keep the first one\n",
        "transactions_v2 = transactions.groupby('msno', as_index = False).agg({'payment_method_id': lambda x:x.value_counts().index[0], 'payment_plan_days': 'max', 'plan_list_price': 'max',\n",
        "                                       'actual_amount_paid': 'max', 'is_auto_renew': lambda x:x.value_counts().index[0], 'transaction_date': 'min', 'membership_expire_date': 'max',\n",
        "                                       'is_cancel': lambda x:x.value_counts().index[0]})\n",
        "\n",
        "# returns the max value of date and number of unique songs\n",
        "# returns the sum of other variables\n",
        "user_logs_v2 = user_logs.groupby('msno', as_index = False).agg({'date': 'max', 'num_25': 'sum', 'num_50': 'sum', 'num_75': 'sum',\n",
        "                                 'num_985': 'sum', 'num_100': 'sum', 'num_unq': 'max', 'total_secs': 'sum'})\n",
        "\n",
        "# calculate the percentage of number of songs played within certain period\n",
        "user_logs_v2['percent_25'] = user_logs_v2['num_25']/(user_logs_v2['num_25']+user_logs_v2['num_50']+user_logs_v2['num_75']+user_logs_v2['num_985']+user_logs_v2['num_100'])\n",
        "user_logs_v2['percent_50'] = user_logs_v2['num_50']/(user_logs_v2['num_25']+user_logs_v2['num_50']+user_logs_v2['num_75']+user_logs_v2['num_985']+user_logs_v2['num_100'])\n",
        "user_logs_v2['percent_100'] = (user_logs_v2['num_985']+user_logs_v2['num_100'])/(user_logs_v2['num_25']+user_logs_v2['num_50']+user_logs_v2['num_75']+user_logs_v2['num_985']+user_logs_v2['num_100'])\n",
        "\n",
        "# drop useless variables\n",
        "user_logs_v3 = user_logs_v2.drop(columns = ['num_25', 'num_50', 'num_75', 'num_985', 'num_100'])\n",
        "\n",
        "# merge between different tables for modelling purpose\n",
        "dataset_train = train.merge(members, on = 'msno', how = 'left').merge(transactions_v2, on = 'msno', how = 'left').merge(user_logs_v3, on = 'msno', how = 'left')\n",
        "dataset_train.dtypes\n",
        "\n",
        "# date in csv will be recognized as float in python\n",
        "# this value needs to be converted back to date\n",
        "dataset_train['registration_init_time'] = pd.to_datetime(dataset_train['registration_init_time'], format = '%Y%m%d')\n",
        "dataset_train['transaction_date'] = pd.to_datetime(dataset_train['transaction_date'], format = '%Y%m%d')\n",
        "dataset_train['membership_expire_date'] = pd.to_datetime(dataset_train['membership_expire_date'], format = '%Y%m%d')\n",
        "dataset_train['date'] = pd.to_datetime(dataset_train['date'], format = '%Y%m%d')\n",
        "\n",
        "# check the maximum of datetime value\n",
        "dataset_train.select_dtypes(include = ['datetime64[ns]']).max()\n",
        "\n",
        "# create new day columns for modelling purpose\n",
        "dataset_train['registration_day'] = (dataset_train['membership_expire_date'].max() - dataset_train['registration_init_time']).astype('timedelta64[D]')\n",
        "dataset_train['transaction_day'] = (dataset_train['membership_expire_date'].max() - dataset_train['transaction_date']).astype('timedelta64[D]')\n",
        "dataset_train['membership_expire_day'] = (dataset_train['membership_expire_date'].max() - dataset_train['membership_expire_date']).astype('timedelta64[D]')\n",
        "dataset_train['last_play_day'] = (dataset_train['membership_expire_date'].max() - dataset_train['date']).astype('timedelta64[D]')\n",
        "\n",
        "# check the distribution of age due to the documentation explanation\n",
        "dataset_train['bd'].value_counts()\n",
        "\n",
        "# remove gender and age since missing value or incorrect value is over 50%\n",
        "dataset_train_v2 = dataset_train.drop(columns = ['msno', 'gender', 'bd', 'registration_init_time', 'transaction_date', 'membership_expire_date', 'date'])\n",
        "dataset_train_v2.dtypes\n",
        "\n",
        "# check the number of missing values in each column\n",
        "dataset_train_v2.isna().sum()\n",
        "\n",
        "# Handle missing value of part of numeric columns by using mode\n",
        "def replacemode(i):\n",
        "    dataset_train_v2[i] = dataset_train_v2[i].fillna(dataset_train_v2[i].value_counts().index[0])\n",
        "    return \n",
        "\n",
        "replacemode('city')\n",
        "replacemode('registered_via')\n",
        "replacemode('payment_method_id')\n",
        "replacemode('payment_plan_days')\n",
        "replacemode('is_auto_renew')\n",
        "replacemode('is_cancel')\n",
        "\n",
        "# Handle missing value of part of numeric columns by using mean\n",
        "from sklearn.impute import SimpleImputer\n",
        "mean_imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
        "def replacemean(i):\n",
        "    dataset_train_v2[i] = mean_imputer.fit_transform(dataset_train_v2[[i]])\n",
        "    return \n",
        "\n",
        "replacemean('plan_list_price')\n",
        "replacemean('actual_amount_paid')\n",
        "replacemean('num_unq')\n",
        "replacemean('total_secs')\n",
        "replacemean('percent_25')\n",
        "replacemean('percent_50')\n",
        "replacemean('percent_100')\n",
        "replacemean('registration_day')\n",
        "replacemean('transaction_day')\n",
        "replacemean('membership_expire_day')\n",
        "replacemean('last_play_day')\n",
        "\n",
        "# Handle outliers by using capping\n",
        "def replaceoutlier(i):\n",
        "    mean, std = np.mean(dataset_train_v2[i]), np.std(dataset_train_v2[i])\n",
        "    cut_off = std*3\n",
        "    lower, upper = mean - cut_off, mean + cut_off\n",
        "    dataset_train_v2[i][dataset_train_v2[i] < lower] = lower\n",
        "    dataset_train_v2[i][dataset_train_v2[i] > upper] = upper\n",
        "    return\n",
        "\n",
        "replaceoutlier('plan_list_price')\n",
        "replaceoutlier('actual_amount_paid')\n",
        "replaceoutlier('num_unq')\n",
        "replaceoutlier('total_secs')\n",
        "replaceoutlier('percent_25')\n",
        "replaceoutlier('percent_50')\n",
        "replaceoutlier('percent_100')\n",
        "replaceoutlier('registration_day')\n",
        "replaceoutlier('transaction_day')\n",
        "replaceoutlier('membership_expire_day')\n",
        "replaceoutlier('last_play_day')\n",
        "\n",
        "dataset_train_v2.dtypes\n",
        "dataset_train_v2.describe()\n",
        "\n",
        "# convert categorical variables into string\n",
        "dataset_train_v2.iloc[:, 1:4] = dataset_train_v2.iloc[:, 1:4].astype(str)\n",
        "\n",
        "# replace discrete features with historical churn rate\n",
        "city_mean = pd.DataFrame(dataset_train_v2.groupby('city')['is_churn'].mean().reset_index())\n",
        "city_mean.rename(columns = {'is_churn': 'city_mean'}, inplace=True)\n",
        "register_mean = pd.DataFrame(dataset_train_v2.groupby('registered_via')['is_churn'].mean().reset_index())\n",
        "register_mean.rename(columns = {'is_churn': 'register_mean'}, inplace=True)\n",
        "payment_mean = pd.DataFrame(dataset_train_v2.groupby('payment_method_id')['is_churn'].mean().reset_index())\n",
        "payment_mean.rename(columns = {'is_churn': 'payment_mean'}, inplace=True)\n",
        "\n",
        "dataset_train_v3 = dataset_train_v2.merge(city_mean, on = 'city', how = 'left').merge(register_mean, on = 'registered_via', how = 'left').merge(payment_mean, on = 'payment_method_id', how = 'left')\n",
        "dataset_train_v3 = dataset_train_v3.drop(columns = ['city', 'registered_via', 'payment_method_id'])\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "X = dataset_train_v3.drop(columns = ['is_churn'])\n",
        "Y = dataset_train_v3['is_churn']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "import sklearn\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0)\n",
        "nm_models_pre = []\n",
        "nm_models_pre.append(('KNN', KNeighborsClassifier()))\n",
        "nm_models_pre.append(('LR', LogisticRegression()))\n",
        "nm_models_pre.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "nm_models_pre.append(('CART', DecisionTreeClassifier()))\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "results = []\n",
        "names = []\n",
        "for name, model in nm_models_pre:\n",
        " nm_cv_results = cross_val_score(model, X_train, Y_train, cv=10, scoring='neg_log_loss', n_jobs = -1)\n",
        " model.fit(X_train,Y_train);\n",
        " y_pred = model.predict(X_test)\n",
        " cm = confusion_matrix(Y_test, y_pred)\n",
        " acc = accuracy_score(Y_test, y_pred)\n",
        " print(cm)\n",
        " print(acc)\n",
        " results.append(nm_cv_results)\n",
        " names.append(name)\n",
        " print(\"Auc-ROC:\")\n",
        " print(roc_auc_score(Y_test, y_pred))\n",
        " msg = \"%s: %f (%f)\" % (name, nm_cv_results.mean(), nm_cv_results.std())\n",
        " print(msg)\n",
        "\t\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "classifier = Sequential()\n",
        "classifier.add(Dense(units = 6, activation = 'relu'))\n",
        "classifier.add(Dense(units = 6, activation = 'relu'))\n",
        "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "classifier.compile(optimizer = 'Adam', loss ='binary_crossentropy', metrics = ['accuracy'])\n",
        "classifier.fit(X_train, Y_train, batch_size = 10, epochs = 5)\n",
        "y_pred = classifier.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(Y_test, y_pred)\n",
        "print(cm)\n",
        "acc = accuracy_score(Y_test, y_pred)\n",
        "print(acc)\n",
        "svc = svm.SVC(C=1, kernel='linear')\n",
        "print(\"Auc-ROC:\")\n",
        "print(roc_auc_score(Y_test, y_pred))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Feature Scaling for modelling purpose by using both min-max-scaling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "nm_X = pd.DataFrame(MinMaxScaler().fit_transform(X))\n",
        "nm_X.columns = X.columns.values\n",
        "nm_X.index = X.index.values\n",
        "\n",
        "# Feature Selection\n",
        "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
        "nm_col = ['is_auto_renew', 'is_cancel']\n",
        "nm_X_v2 = nm_X.drop(columns = nm_col)\n",
        "nm_X_v3 = pd.DataFrame(nm_X, columns = nm_col)\n",
        "nm_X_v4 = pd.DataFrame(SelectKBest(score_func=chi2, k='all').fit(nm_X_v3, Y).pvalues_ <= 0.05, columns = ['importance'])\n",
        "nm_X_v4.index = nm_X_v3.columns.values\n",
        "nm_X_v5 = pd.DataFrame(SelectKBest(score_func=f_classif, k='all').fit(nm_X_v2, Y).pvalues_ <= 0.05, columns = ['importance'])\n",
        "nm_X_v5.index = nm_X_v2.columns.values\n",
        "nm_X_v6 = pd.concat([nm_X_v4, nm_X_v5])\n",
        "nm_selected = list(pd.Series(nm_X_v6[nm_X_v6['importance'] == 1].index.values))\n",
        "nm_X_v7 = pd.DataFrame(nm_X, columns = nm_selected)\n",
        "\n",
        "# Dimension Reduction\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "pca.fit_transform(nm_X_v7)\n",
        "np.cumsum(pca.explained_variance_ratio_)\n",
        "nm_X_v8 = PCA(n_components=10).fit_transform(nm_X_v7)\n",
        "\n",
        "# Split into train and test Set\n",
        "from sklearn.model_selection import train_test_split\n",
        "nm_X_train, nm_X_test, nm_Y_train, nm_Y_test = train_test_split(nm_X_v8, Y, test_size = 0.3, random_state = 0)\n",
        "\n",
        "# Fit training set into different algorithms\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "nm_models = []\n",
        "\n",
        "nm_models.append(('LR', LogisticRegression()))\n",
        "nm_models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "\n",
        "ensembles = []\n",
        "\n",
        "ensembles.append(('XGB', XGBClassifier()))\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "results = []\n",
        "names = []\n",
        "for name, model in nm_models:\n",
        "  nm_cv_results = cross_val_score(model, nm_X_train, nm_Y_train, cv=10, scoring='neg_log_loss', n_jobs = -1)\n",
        "  model.fit(nm_X_train,nm_Y_train)\n",
        "  results.append(nm_cv_results)\n",
        "  nm_y_pred = model.predict(nm_X_test)\n",
        "  cm = confusion_matrix(nm_Y_test, nm_y_pred)\n",
        "  acc = accuracy_score(nm_Y_test, nm_y_pred)\n",
        "  print(acc)\n",
        "  print(cm)\n",
        "  names.append(name)\n",
        "  print(\"AUC-ROC:\")\n",
        "  print(roc_auc_score(nm_Y_test, nm_y_pred))\n",
        "  msg = \"%s: %f (%f)\" % (name, nm_cv_results.mean(), nm_cv_results.std())\n",
        "  print(msg)\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "nm_X_train = sc.fit_transform(nm_X_train)\n",
        "nm_X_test = sc.transform(nm_X_test)\n",
        "classifier = Sequential()\n",
        "classifier.add(Dense(units = 6, activation = 'relu'))\n",
        "classifier.add(Dense(units = 6, activation = 'relu'))\n",
        "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "classifier.compile(optimizer = 'Adam', loss ='binary_crossentropy', metrics = ['accuracy'])\n",
        "classifier.fit(nm_X_train, nm_Y_train, batch_size = 10, epochs = 5)\n",
        "y_pred = classifier.predict(nm_X_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(nm_Y_test, y_pred)\n",
        "print(cm)\n",
        "acc = accuracy_score(nm_Y_test, y_pred)\n",
        "print(acc)\n",
        "svc = svm.SVC(C=1, kernel='linear')\n",
        "print(\"AUC-ROC:\")\n",
        "print(roc_auc_score(nm_Y_test, nm_y_pred))\n",
        "\t\n",
        "\t\n",
        "    \n",
        "results2 = []\n",
        "names2 = []\n",
        "for name2, model2 in ensembles:\n",
        "  en_cv_results = cross_val_score(model2, nm_X_train, nm_Y_train, cv=10, scoring='neg_log_loss', n_jobs = -1)\n",
        "  results2.append(en_cv_results)\n",
        "  names2.append(name2)\n",
        "  nm_y_pred = model.predict(nm_X_test)\n",
        "  cm = confusion_matrix(nm_Y_test, nm_y_pred)\n",
        "  acc = accuracy_score(nm_Y_test, nm_y_pred)\n",
        "  print(acc)\n",
        "  print(cm)\n",
        "  print(\"AUC-ROC:\")\n",
        "  print(roc_auc_score(nm_Y_test, nm_y_pred))\n",
        "  msg2 = \"%s: %f (%f)\" % (name2, en_cv_results.mean(), en_cv_results.std())\n",
        "  print(msg2)\n",
        "\n",
        "# Apply Grid Search on XGBoost since it returns the best result on Cross Validation among all models\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters = {\"learning_rate\": [0.1],\n",
        "              \"min_child_weight\": [1],\n",
        "              \"max_depth\": [9],\n",
        "              \"gamma\": [0.1],\n",
        "              \"subsample\": [0.8],\n",
        "              \"colsample_bytree\": [0.8],\n",
        "              \"objective\": ['binary:logistic']}\n",
        "grid_search_XGB = GridSearchCV(estimator = XGBClassifier(), param_grid = parameters, scoring = \"neg_log_loss\", cv = 10, n_jobs = -1)\n",
        "grid_result_XGB = grid_search_XGB.fit(nm_X_train, nm_Y_train)\n",
        "print(\"Best: %f using %s\" % (grid_result_XGB.best_score_, grid_result_XGB.best_params_)) \n",
        "\n",
        "# Evaluate tuned XGBoost model result on test dataset because it provides the best result\n",
        "from sklearn.metrics import log_loss\n",
        "nm_Y_predict = grid_result_XGB.predict_proba(nm_X_test)\n",
        "logloss = log_loss(nm_Y_test, nm_Y_predict)  \n",
        "print(\"Log Loss Score: %s\" % (logloss))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[263303   1789]\n",
            " [ 16554   9642]]\n",
            "0.9370279585839444\n",
            "Auc-ROC:\n",
            "0.6806614304029657\n",
            "KNN: -1.248614 (0.008413)\n",
            "[[263911   1181]\n",
            " [ 20372   5824]]\n",
            "0.9260079371618467\n",
            "Auc-ROC:\n",
            "0.6089344809109704\n",
            "LR: -0.243184 (0.001616)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[260598   4494]\n",
            " [ 13810  12386]]\n",
            "0.9371618466946802\n",
            "Auc-ROC:\n",
            "0.7279338363858557\n",
            "LDA: -0.268407 (0.004003)\n",
            "[[260963   4129]\n",
            " [  4151  22045]]\n",
            "0.9715745241822527\n",
            "Auc-ROC:\n",
            "0.9129824842907631\n",
            "CART: -0.974079 (0.018561)\n",
            "Epoch 1/5\n",
            "67968/67968 [==============================] - 109s 1ms/step - loss: 0.1280 - accuracy: 0.9521\n",
            "Epoch 2/5\n",
            "67968/67968 [==============================] - 92s 1ms/step - loss: 0.0896 - accuracy: 0.9688\n",
            "Epoch 3/5\n",
            "67968/67968 [==============================] - 91s 1ms/step - loss: 0.0884 - accuracy: 0.9693\n",
            "Epoch 4/5\n",
            "67968/67968 [==============================] - 95s 1ms/step - loss: 0.0875 - accuracy: 0.9697\n",
            "Epoch 5/5\n",
            "67968/67968 [==============================] - 90s 1ms/step - loss: 0.0868 - accuracy: 0.9699\n",
            "[[260902   4190]\n",
            " [  4543  21653]]\n",
            "0.9700193622806295\n",
            "Auc-ROC:\n",
            "0.9053853715650375\n",
            "0.9351432259482024\n",
            "[[261180   3912]\n",
            " [ 14980  11216]]\n",
            "AUC-ROC:\n",
            "0.7066999148063683\n",
            "LR: -0.178730 (0.001039)\n",
            "0.9359465546126171\n",
            "[[260637   4455]\n",
            " [ 14203  11993]]\n",
            "AUC-ROC:\n",
            "0.7205062505409146\n",
            "LDA: -0.270151 (0.003567)\n",
            "Epoch 1/5\n",
            "67968/67968 [==============================] - 92s 1ms/step - loss: 0.1996 - accuracy: 0.9367\n",
            "Epoch 2/5\n",
            "67968/67968 [==============================] - 88s 1ms/step - loss: 0.1029 - accuracy: 0.9624\n",
            "Epoch 3/5\n",
            "67968/67968 [==============================] - 87s 1ms/step - loss: 0.0960 - accuracy: 0.9661\n",
            "Epoch 4/5\n",
            "67968/67968 [==============================] - 89s 1ms/step - loss: 0.0932 - accuracy: 0.9671\n",
            "Epoch 5/5\n",
            "67968/67968 [==============================] - 98s 1ms/step - loss: 0.0935 - accuracy: 0.9670\n",
            "[[259500   5592]\n",
            " [  4358  21838]]\n",
            "0.9658413666199774\n",
            "AUC-ROC:\n",
            "0.7205062505409146\n",
            "0.8891955727664717\n",
            "[[240350  24742]\n",
            " [  7534  18662]]\n",
            "AUC-ROC:\n",
            "0.8095326022010637\n",
            "XGB: -0.100784 (0.001614)\n",
            "Best: -0.076631 using {'colsample_bytree': 0.8, 'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 9, 'min_child_weight': 1, 'objective': 'binary:logistic', 'subsample': 0.8}\n",
            "Log Loss Score: 0.07638962554685129\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}